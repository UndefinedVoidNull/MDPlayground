以下是人工神经网络中常见的激活函数，以及它们的讲解、比较和实际例子：

---

### **1. Sigmoid Function (S形函数)**  
公式：  
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

#### **特点**：
- 输出范围：$(0, 1)$
- 主要用于二分类问题的概率输出。
- 让输入值变为“平滑的0到1之间的值”。

#### **优点**：
- 简单易用，输出可解释为概率。

#### **缺点**：
- **梯度消失问题**：当输入很大或很小时，梯度几乎为0，导致权重更新很慢。
- **非零均值问题**：输出值不以0为中心，可能导致较慢的训练。

#### **应用场景**：
- 用于输出层进行二分类概率输出。

#### **例子**：
在逻辑回归中：
$$
\hat{y} = \sigma(w \cdot x + b)
$$

---

### **2. Tanh Function (双曲正切函数)**  
公式：  
$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

#### **特点**：
- 输出范围：$(-1, 1)$
- 对称于原点（零均值），比 Sigmoid 更适合深度网络。

#### **优点**：
- 负值对梯度变化敏感，效果通常优于 Sigmoid。

#### **缺点**：
- **梯度消失问题**：输入过大或过小时，梯度几乎为0。

#### **应用场景**：
- 需要归一化到正负范围的中间层输出。

#### **例子**：
神经网络隐藏层：
$$
z = w \cdot x + b,\quad a = \tanh(z)
$$

---

### **3. ReLU (Rectified Linear Unit, 线性整流函数)**  
公式：  
$$
f(x) = \max(0, x)
$$

#### **特点**：
- 输出范围：$[0, +\infty)$
- 非线性，但计算简单。

#### **优点**：
- 计算高效，不需要复杂运算（如指数）。
- 减轻梯度消失问题，适合深度网络。

#### **缺点**：
- **Dying ReLU 问题**：如果输入始终小于0，梯度为0，神经元可能无法恢复。

#### **应用场景**：
- 深度卷积神经网络（CNN）中常用的激活函数。

#### **例子**：
深度学习隐藏层：
$$
z = w \cdot x + b,\quad a = \max(0, z)
$$

---

### **4. Leaky ReLU (带泄漏的ReLU)**  
公式：  
$$
f(x) = \begin{cases} 
x, & x > 0 \\
\alpha x, & x \leq 0
\end{cases}
$$
其中 $\alpha$ 是一个很小的常数（如 0.01）。

#### **特点**：
- 输出范围：$(-\infty, +\infty)$
- 类似于 ReLU，但避免了 Dying ReLU 问题。

#### **优点**：
- 负输入也有梯度，不会让神经元完全失活。

#### **缺点**：
- 引入了超参数 $\alpha$，需要调节。

#### **应用场景**：
- 深度神经网络中用于替代标准 ReLU。

#### **例子**：
$$
z = w \cdot x + b,\quad a = \text{LeakyReLU}(z)
$$

---

### **5. Softmax Function**  
公式：  
$$
\sigma(x)_i = \frac{e^{x_i}}{\sum_{j} e^{x_j}}
$$

#### **特点**：
- 输出范围：$(0, 1)$
- 将向量转为概率分布，适用于多分类问题。

#### **优点**：
- 每个输出都在 0 到 1 之间，并且总和为1。

#### **缺点**：
- 对数值范围敏感，可能需要归一化处理。

#### **应用场景**：
- 多分类问题的输出层。

#### **例子**：
用于多分类：
$$
\hat{y} = \text{Softmax}(Wx + b)
$$

---

### **6. Swish**  
公式：  
$$
f(x) = x \cdot \sigma(x) = x \cdot \frac{1}{1 + e^{-x}}
$$

#### **特点**：
- 输出范围：$(-\infty, +\infty)$
- 由 Google 提出，兼具 ReLU 和 Sigmoid 的优点。

#### **优点**：
- 非单调性带来更灵活的表达能力。
- 在某些任务中表现优于 ReLU。

#### **缺点**：
- 计算稍复杂，训练时间较长。

#### **应用场景**：
- 高效神经网络模型（如EfficientNet）。

#### **例子**：
$$
z = w \cdot x + b,\quad a = z \cdot \sigma(z)
$$

---

### **比较总结**：

| 激活函数   | 输出范围      | 是否有梯度消失问题 | 计算复杂度 | 常见应用场景                  |
|------------|---------------|--------------------|------------|-------------------------------|
| Sigmoid    | $(0, 1)$  | 是                 | 中         | 二分类问题输出层             |
| Tanh       | $(-1, 1)$ | 是                 | 中         | 隐藏层输出                   |
| ReLU       | $[0, \infty)$ | 否（但有Dying问题） | 低         | 深度学习主流激活函数         |
| Leaky ReLU | $(-\infty, \infty)$ | 否                 | 低         | 替代标准ReLU                |
| Softmax    | $(0, 1)$  | 否                 | 高         | 多分类输出层                 |
| Swish      | $(-\infty, \infty)$ | 否                 | 中         | 新型神经网络中的创新激活函数 |

---

### 实际例子：

假设一个简单的三层神经网络：
1. 输入层：图像像素值。
2. 隐藏层1：使用 ReLU。
3. 隐藏层2：使用 Tanh。
4. 输出层：使用 Softmax。

前向传播计算：
$$
h_1 = \text{ReLU}(W_1 \cdot X + b_1)
$$
$$
h_2 = \tanh(W_2 \cdot h_1 + b_2)
$$
$$
\hat{y} = \text{Softmax}(W_3 \cdot h_2 + b_3)
$$